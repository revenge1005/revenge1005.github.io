---
title:  "[Retry, k8s] 06. ì˜¨í”„ë ˆë¯¸ìŠ¤ í™˜ê²½ì—ì„œ SDSì— ì˜í•œ ë™ì  ë³¼ë¥¨ í”„ë¡œë¹„ì €ë‹(CephFS) - (2)"

categories:
  - KUBERNETES
tags:
  - [kubernetes]

toc: true
toc_sticky: true

date: 2023-03-01
last_modified_at: 2023-03-01
---
# [Retry, k8s] 06. ì˜¨í”„ë ˆë¯¸ìŠ¤ í™˜ê²½ì—ì„œ SDSì— ì˜í•œ ë™ì  ë³¼ë¥¨ í”„ë¡œë¹„ì €ë‹(CephFS) - (2)
---

<style>
table {
    font-size: 12pt;
}
table th:first-of-type {
    width: 5%;
}
table th:nth-of-type(2) {
    width: 15%;
}
table th:nth-of-type(3) {
    width: 50%;
}
table th:nth-of-type(4) {
    width: 30%;
}
</style>

<br>

## ğŸ”” Ceph í´ëŸ¬ìŠ¤í„° êµ¬ì„±


![sss](https://user-images.githubusercontent.com/42735894/228226470-61b5ddc7-a4a3-4e42-9532-e9ad89fa9731.png){: width="100%" height="100%"}{: .align-center}


![21e2qweqw](https://user-images.githubusercontent.com/42735894/228226419-91dae4b5-2978-4855-96ed-11d7f18ef018.PNG){: width="100%" height="100%"}{: .align-center}


### (1) [Monitor+Manager ë°ëª¬] ë…¸ë“œ(ì—¬ê¸°ì„œëŠ” ê´€ë¦¬ ë…¸ë“œë¼ê³  í•¨)ì—ì„œ SSH í‚¤ ìŒì„ ìƒì„±í•˜ê³  ê° ë…¸ë“œì— ì„¤ì •

```bash
root@osd01:~# cat <<EOF >> /etc/hosts

192.168.219.51 osd01 osd01.test.com
192.168.219.52 osd02 osd02.test.com
192.168.219.53 osd03 osd03.test.com
EOF


root@osd01:~# {
    ssh-keygen -t ed25519 -C "Ansible Test" -N ''
	for i in osd01 osd02 osd03;
	do
	    ssh-copy-id -i ~/.ssh/id_ed25519.pub $i
	done
}
```


### (2) ê´€ë¦¬ ë…¸ë“œì—ì„œ ê° ë…¸ë“œì— Cephë¥¼ ì„¤ì¹˜

```bash
root@osd01:~# for NODE in osd01 osd02 osd03
do
    ssh $NODE "apt update; apt -y install ceph"
done
```


### (3) ê´€ë¦¬ ë…¸ë“œì—ì„œ [Monitor+Manager ë°ëª¬]ì„ ì„¤ì •

```bash
# UUID ìƒì„±
root@osd01:~# uuidgen
3adf5d85-7d69-455d-82cf-f799e63981e4

# ìƒˆ êµ¬ì„± ìƒì„±
# íŒŒì¼ ì´ë¦„ â‡’ (ì„ì˜ì˜ í´ëŸ¬ìŠ¤í„° ì´ë¦„).conf
# ì´ ì˜ˆì—ì„œ í´ëŸ¬ìŠ¤í„° ì´ë¦„ [ceph](ê¸°ë³¸ê°’) ì„¤ì • Â» [ceph.conf]
cat <<EOF > /etc/ceph/ceph.conf

[global]
# ëª¨ë‹ˆí„°ë§í•  í´ëŸ¬ìŠ¤í„° ë„¤íŠ¸ì›Œí¬ ì§€ì •
cluster network = 192.168.219.0/24
# ê³µìš© ë„¤íŠ¸ì›Œí¬ ì§€ì •
public network = 192.168.219.0/24
# ìœ„ì—ì„œ ìƒì„±ëœ UUID ì§€ì •
fsid = 3adf5d85-7d69-455d-82cf-f799e63981e4
# ëª¨ë‹ˆí„° ë°ëª¬ì˜ IP ì£¼ì†Œ ì§€ì •
mon host = 192.168.219.51
# ëª¨ë‹ˆí„° ë°ëª¬ì˜ í˜¸ìŠ¤íŠ¸ ì´ë¦„ ì§€ì •
mon initial members = osd01 
osd pool default crush rule = -1

# mon.(ë…¸ë“œ ì´ë¦„)
[mon.osd01]
# ëª¨ë‹ˆí„° ë°ëª¬ì˜ í˜¸ìŠ¤íŠ¸ ì´ë¦„ ì§€ì •
host = osd01
# ëª¨ë‹ˆí„° ë°ëª¬ì˜ IP ì£¼ì†Œ ì§€ì •
mon addr = 192.168.219.51
# í’€ ì‚­ì œ í—ˆìš©
mon allow pool delete = true
EOF
```

```bash
# ë¹„ë°€ í‚¤ ìƒì„±
root@osd01:~# {
    echo '# í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë¹„ë°€ í‚¤ ìƒì„±'
    ceph-authtool --create-keyring /etc/ceph/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'
    echo '# í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ìì— ëŒ€í•œ ë¹„ë°€ í‚¤ ìƒì„±'
    ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'
    echo '# ë¶€íŠ¸ìŠ¤íŠ¸ë©ìš© í‚¤ ìƒì„±'
    ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon 'profile bootstrap-osd' --cap mgr 'allow r'
}


# ìƒì„±ëœ í‚¤ ê°€ì ¸ì˜¤ê¸°
root@osd01:~# {
    ceph-authtool /etc/ceph/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
	ceph-authtool /etc/ceph/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring
}


# ëª¨ë‹ˆí„°ë§µ ìƒì„±
root@osd01:~# {
    FSID=$(grep "^fsid" /etc/ceph/ceph.conf | awk {'print $NF'})
	NODENAME=$(grep "^mon initial" /etc/ceph/ceph.conf | awk {'print $NF'})
	NODEIP=$(grep "^mon host" /etc/ceph/ceph.conf | awk {'print $NF'})
	monmaptool --create --add $NODENAME $NODEIP --fsid $FSID /etc/ceph/monmap
}
```

```bash
# ëª¨ë‹ˆí„° ë°ëª¬ì— ëŒ€í•œ ë””ë ‰í„°ë¦¬ ìƒì„± / í‚¤ ë° ëª¨ë‹ˆí„° ë§µì„ ëª¨ë‹ˆí„° ë°ëª¬ì— ì—°ê²°
# ë””ë ‰í† ë¦¬ ì´ë¦„ â‡’ (í´ëŸ¬ìŠ¤í„° ì´ë¦„)-(ë…¸ë“œ ì´ë¦„)
# --cluster (í´ëŸ¬ìŠ¤í„° ì´ë¦„)
root@osd01:~# mkdir /var/lib/ceph/mon/ceph-osd01


root@osd01:~# {
    ceph-mon --cluster ceph --mkfs -i $NODENAME --monmap /etc/ceph/monmap --keyring /etc/ceph/ceph.mon.keyring
	chown ceph. /etc/ceph/ceph.*
	chown -R ceph. /var/lib/ceph/mon/ceph-osd01 /var/lib/ceph/bootstrap-osd
	systemctl enable --now ceph-mon@$NODENAME
}
```

```bash
# Messenger v2 í”„ë¡œí† ì½œ ì‚¬ìš©
root@osd01:~# {
	ceph mon enable-msgr2
	ceph config set mon auth_allow_insecure_global_id_reclaim false
}
```

```bash
# Manager Daemonì˜ ë””ë ‰í„°ë¦¬ ìƒì„±
# ë””ë ‰í† ë¦¬ ì´ë¦„ â‡’ (í´ëŸ¬ìŠ¤í„° ì´ë¦„)-(ë…¸ë“œ ì´ë¦„)
root@osd01:~# mkdir /var/lib/ceph/mgr/ceph-osd01


# ì¸ì¦ í‚¤ ìƒì„±
root@osd01:~# {
    ceph auth get-or-create mgr.$NODENAME mon 'allow profile mgr' osd 'allow *' mds 'allow *'
	ceph auth get-or-create mgr.osd01 | tee /etc/ceph/ceph.mgr.admin.keyring
	cp /etc/ceph/ceph.mgr.admin.keyring /var/lib/ceph/mgr/ceph-osd01/keyring
	chown ceph. /etc/ceph/ceph.mgr.admin.keyring
	chown -R ceph. /var/lib/ceph/mgr/ceph-osd01
	systemctl enable --now ceph-mgr@$NODENAME
}
```


### (4) í´ëŸ¬ìŠ¤í„° ìƒíƒœë¥¼ í™•ì¸

```bash
root@osd01:~# ceph -s
  cluster:
    id:     3adf5d85-7d69-455d-82cf-f799e63981e4
    health: HEALTH_WARN
            OSD count 0 < osd_pool_default_size 3

  services:
    mon: 1 daemons, quorum osd01 (age 3m)
    mgr: osd01(active, since 2m)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:
```


### (5) ê´€ë¦¬ ë…¸ë“œì—ì„œ ê° ë…¸ë“œë¡œ OSD(Object Storage Device)ë¥¼ êµ¬ì„± 

```bash
root@osd01:~# for NODE in osd01 osd02 osd03
do
    if [ ! ${NODE} = "osd01" ]
    then
        scp /etc/ceph/ceph.conf ${NODE}:/etc/ceph/ceph.conf
        scp /etc/ceph/ceph.client.admin.keyring ${NODE}:/etc/ceph
        scp /var/lib/ceph/bootstrap-osd/ceph.keyring ${NODE}:/var/lib/ceph/bootstrap-osd
    fi
    ssh $NODE \
    "chown ceph. /etc/ceph/ceph.* /var/lib/ceph/bootstrap-osd/*; \
    parted --script /dev/sdb 'mklabel gpt'; \
    parted --script /dev/sdb "mkpart primary 0% 100%"; \
    ceph-volume lvm create --data /dev/sdb1"
done 
```


### (6) í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸

```bash
root@osd01:~# ceph -s
  cluster:
    id:     3adf5d85-7d69-455d-82cf-f799e63981e4
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum osd01 (age 9m)
    mgr: osd01(active, since 8m)
    osd: 3 osds: 3 up (since 74s), 3 in (since 3m)

  data:
    pools:   1 pools, 1 pgs
    objects: 2 objects, 449 KiB
    usage:   61 MiB used, 120 GiB / 120 GiB avail
    pgs:     1 active+clean
```


### (7) OSD íŠ¸ë¦¬ í™•ì¸

```bash
root@osd01:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.46857  root default
-3         0.15619      host node01
 0    hdd  0.15619          osd.0        up   1.00000  1.00000
-5         0.15619      host node02
 1    hdd  0.15619          osd.1        up   1.00000  1.00000
-7         0.15619      host node03
 2    hdd  0.15619          osd.2        up   1.00000  1.00000


root@osd01:~# ceph df 
--- RAW STORAGE ---
CLASS     SIZE    AVAIL    USED  RAW USED  %RAW USED
hdd    120 GiB  120 GiB  61 MiB    61 MiB       0.01
TOTAL  120 GiB  120 GiB  61 MiB    61 MiB       0.01

--- POOLS ---
POOL  ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr   1    1  389 KiB        2  1.1 MiB      0    152 GiB


root@osd01:~# ceph osd df 
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META    AVAIL    %USE  VAR   PGS  STATUS
 0    hdd  0.15619   1.00000  40 GiB   20 MiB  544 KiB   0 B  20 MiB   40 GiB  0.01  1.00    1      up
 1    hdd  0.15619   1.00000  40 GiB   20 MiB  544 KiB   0 B  20 MiB   40 GiB  0.01  1.00    1      up
 2    hdd  0.15619   1.00000  40 GiB   20 MiB  540 KiB   0 B  20 MiB   40 GiB  0.01  1.00    1      up
                      TOTAL  120 GiB   61 MiB  1.6 MiB   0 B  59 MiB  120 GiB  0.01
MIN/MAX VAR: 1.00/1.00  STDDEV: 0
```

### (8) ê´€ë¦¬ ë…¸ë“œì—ì„œ MDS(ë©”íƒ€ ë°ì´í„° ì„œë²„)ë¥¼ êµ¬ì„±

```bash
# ë””ë ‰í„°ë¦¬ ë§Œë“¤ê¸°
# ë””ë ‰í† ë¦¬ ì´ë¦„ â‡’ (í´ëŸ¬ìŠ¤í„° ì´ë¦„)-(ë…¸ë“œ ì´ë¦„)
root@osd01:~# mkdir -p /var/lib/ceph/mds/ceph-osd01


root@osd01:~# ceph-authtool --create-keyring /var/lib/ceph/mds/ceph-osd01/keyring --gen-key -n mds.osd01


root@osd01:~# chown -R ceph. /var/lib/ceph/mds/ceph-osd01


root@osd01:~# ceph auth add mds.osd01 osd "allow rwx" mds "allow" mon "allow profile mds" -i /var/lib/ceph/mds/ceph-osd01/keyring


root@osd01:~# systemctl enable --now ceph-mds@osd01
```


### (9) Ceph fs volume ìƒì„±

```bash
root@osd01:~# ceph fs volume create kubernetes
Volume created successfully (no MDS daemons created)

root@osd01:~# ceph fs ls
name: kubernetes, metadata pool: cephfs.kubernetes.meta, data pools: [cephfs.kubernetes.data ]
```


### (10) ceph fs auth ìƒì„±

```bash
root@osd01:~# ceph auth get-or-create client.cephfs mon 'allow r' osd 'allow rwx pool=kubernetes'
[client.cephfs]
    key = AQBgJ1Rg9DMzEhAA/y3+g2tDGHOujxPVjFHS6A==
```


### (11) ceph mds stat

```bash
root@osd01:~# ceph mds stat
kubernetes:1 cephfs:1 {cephfs:0=osd01=up:active}
```

<br>