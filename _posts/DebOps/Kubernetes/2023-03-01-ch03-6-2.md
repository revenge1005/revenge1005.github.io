---
title:  "[Retry, k8s] 06. 온프레미스 환경에서 SDS에 의한 동적 볼륨 프로비저닝(CephFS) - (2)"

categories:
  - KUBERNETES
tags:
  - [kubernetes]

toc: true
toc_sticky: true

date: 2023-03-01
last_modified_at: 2023-03-01
---
# [Retry, k8s] 06. 온프레미스 환경에서 SDS에 의한 동적 볼륨 프로비저닝(CephFS) - (2)
---

<style>
table {
    font-size: 12pt;
}
table th:first-of-type {
    width: 5%;
}
table th:nth-of-type(2) {
    width: 15%;
}
table th:nth-of-type(3) {
    width: 50%;
}
table th:nth-of-type(4) {
    width: 30%;
}
</style>

<br>

## 🔔 Ceph 클러스터 구성


![sss](https://user-images.githubusercontent.com/42735894/228226470-61b5ddc7-a4a3-4e42-9532-e9ad89fa9731.png){: width="100%" height="100%"}{: .align-center}


![21e2qweqw](https://user-images.githubusercontent.com/42735894/228226419-91dae4b5-2978-4855-96ed-11d7f18ef018.PNG){: width="100%" height="100%"}{: .align-center}


### (1) [Monitor+Manager 데몬] 노드(여기서는 관리 노드라고 함)에서 SSH 키 쌍을 생성하고 각 노드에 설정

```bash
root@osd01:~# cat <<EOF >> /etc/hosts

192.168.219.51 osd01 osd01.test.com
192.168.219.52 osd02 osd02.test.com
192.168.219.53 osd03 osd03.test.com
EOF


root@osd01:~# {
    ssh-keygen -t ed25519 -C "Ansible Test" -N ''
	for i in osd01 osd02 osd03;
	do
	    ssh-copy-id -i ~/.ssh/id_ed25519.pub $i
	done
}
```


### (2) 관리 노드에서 각 노드에 Ceph를 설치

```bash
root@osd01:~# for NODE in osd01 osd02 osd03
do
    ssh $NODE "apt update; apt -y install ceph"
done
```


### (3) 관리 노드에서 [Monitor+Manager 데몬]을 설정

```bash
# UUID 생성
root@osd01:~# uuidgen
3adf5d85-7d69-455d-82cf-f799e63981e4

# 새 구성 생성
# 파일 이름 ⇒ (임의의 클러스터 이름).conf
# 이 예에서 클러스터 이름 [ceph](기본값) 설정 » [ceph.conf]
cat <<EOF > /etc/ceph/ceph.conf

[global]
# 모니터링할 클러스터 네트워크 지정
cluster network = 192.168.219.0/24
# 공용 네트워크 지정
public network = 192.168.219.0/24
# 위에서 생성된 UUID 지정
fsid = 3adf5d85-7d69-455d-82cf-f799e63981e4
# 모니터 데몬의 IP 주소 지정
mon host = 192.168.219.51
# 모니터 데몬의 호스트 이름 지정
mon initial members = osd01 
osd pool default crush rule = -1

# mon.(노드 이름)
[mon.osd01]
# 모니터 데몬의 호스트 이름 지정
host = osd01
# 모니터 데몬의 IP 주소 지정
mon addr = 192.168.219.51
# 풀 삭제 허용
mon allow pool delete = true
EOF
```

```bash
# 비밀 키 생성
root@osd01:~# {
    echo '# 클러스터 모니터링을 위한 비밀 키 생성'
    ceph-authtool --create-keyring /etc/ceph/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'
    echo '# 클러스터 관리자에 대한 비밀 키 생성'
    ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'
    echo '# 부트스트랩용 키 생성'
    ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon 'profile bootstrap-osd' --cap mgr 'allow r'
}


# 생성된 키 가져오기
root@osd01:~# {
    ceph-authtool /etc/ceph/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
	ceph-authtool /etc/ceph/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring
}


# 모니터맵 생성
root@osd01:~# {
    FSID=$(grep "^fsid" /etc/ceph/ceph.conf | awk {'print $NF'})
	NODENAME=$(grep "^mon initial" /etc/ceph/ceph.conf | awk {'print $NF'})
	NODEIP=$(grep "^mon host" /etc/ceph/ceph.conf | awk {'print $NF'})
	monmaptool --create --add $NODENAME $NODEIP --fsid $FSID /etc/ceph/monmap
}
```

```bash
# 모니터 데몬에 대한 디렉터리 생성 / 키 및 모니터 맵을 모니터 데몬에 연결
# 디렉토리 이름 ⇒ (클러스터 이름)-(노드 이름)
# --cluster (클러스터 이름)
root@osd01:~# mkdir /var/lib/ceph/mon/ceph-osd01


root@osd01:~# {
    ceph-mon --cluster ceph --mkfs -i $NODENAME --monmap /etc/ceph/monmap --keyring /etc/ceph/ceph.mon.keyring
	chown ceph. /etc/ceph/ceph.*
	chown -R ceph. /var/lib/ceph/mon/ceph-osd01 /var/lib/ceph/bootstrap-osd
	systemctl enable --now ceph-mon@$NODENAME
}
```

```bash
# Messenger v2 프로토콜 사용
root@osd01:~# {
	ceph mon enable-msgr2
	ceph config set mon auth_allow_insecure_global_id_reclaim false
}
```

```bash
# Manager Daemon의 디렉터리 생성
# 디렉토리 이름 ⇒ (클러스터 이름)-(노드 이름)
root@osd01:~# mkdir /var/lib/ceph/mgr/ceph-osd01


# 인증 키 생성
root@osd01:~# {
    ceph auth get-or-create mgr.$NODENAME mon 'allow profile mgr' osd 'allow *' mds 'allow *'
	ceph auth get-or-create mgr.osd01 | tee /etc/ceph/ceph.mgr.admin.keyring
	cp /etc/ceph/ceph.mgr.admin.keyring /var/lib/ceph/mgr/ceph-osd01/keyring
	chown ceph. /etc/ceph/ceph.mgr.admin.keyring
	chown -R ceph. /var/lib/ceph/mgr/ceph-osd01
	systemctl enable --now ceph-mgr@$NODENAME
}
```


### (4) 클러스터 상태를 확인

```bash
root@osd01:~# ceph -s
  cluster:
    id:     3adf5d85-7d69-455d-82cf-f799e63981e4
    health: HEALTH_WARN
            OSD count 0 < osd_pool_default_size 3

  services:
    mon: 1 daemons, quorum osd01 (age 3m)
    mgr: osd01(active, since 2m)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:
```


### (5) 관리 노드에서 각 노드로 OSD(Object Storage Device)를 구성 

```bash
root@osd01:~# for NODE in osd01 osd02 osd03
do
    if [ ! ${NODE} = "osd01" ]
    then
        scp /etc/ceph/ceph.conf ${NODE}:/etc/ceph/ceph.conf
        scp /etc/ceph/ceph.client.admin.keyring ${NODE}:/etc/ceph
        scp /var/lib/ceph/bootstrap-osd/ceph.keyring ${NODE}:/var/lib/ceph/bootstrap-osd
    fi
    ssh $NODE \
    "chown ceph. /etc/ceph/ceph.* /var/lib/ceph/bootstrap-osd/*; \
    parted --script /dev/sdb 'mklabel gpt'; \
    parted --script /dev/sdb "mkpart primary 0% 100%"; \
    ceph-volume lvm create --data /dev/sdb1"
done 
```


### (6) 클러스터 상태 확인

```bash
root@osd01:~# ceph -s
  cluster:
    id:     3adf5d85-7d69-455d-82cf-f799e63981e4
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum osd01 (age 9m)
    mgr: osd01(active, since 8m)
    osd: 3 osds: 3 up (since 74s), 3 in (since 3m)

  data:
    pools:   1 pools, 1 pgs
    objects: 2 objects, 449 KiB
    usage:   61 MiB used, 120 GiB / 120 GiB avail
    pgs:     1 active+clean
```


### (7) OSD 트리 확인

```bash
root@osd01:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.46857  root default
-3         0.15619      host node01
 0    hdd  0.15619          osd.0        up   1.00000  1.00000
-5         0.15619      host node02
 1    hdd  0.15619          osd.1        up   1.00000  1.00000
-7         0.15619      host node03
 2    hdd  0.15619          osd.2        up   1.00000  1.00000


root@osd01:~# ceph df 
--- RAW STORAGE ---
CLASS     SIZE    AVAIL    USED  RAW USED  %RAW USED
hdd    120 GiB  120 GiB  61 MiB    61 MiB       0.01
TOTAL  120 GiB  120 GiB  61 MiB    61 MiB       0.01

--- POOLS ---
POOL  ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr   1    1  389 KiB        2  1.1 MiB      0    152 GiB


root@osd01:~# ceph osd df 
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META    AVAIL    %USE  VAR   PGS  STATUS
 0    hdd  0.15619   1.00000  40 GiB   20 MiB  544 KiB   0 B  20 MiB   40 GiB  0.01  1.00    1      up
 1    hdd  0.15619   1.00000  40 GiB   20 MiB  544 KiB   0 B  20 MiB   40 GiB  0.01  1.00    1      up
 2    hdd  0.15619   1.00000  40 GiB   20 MiB  540 KiB   0 B  20 MiB   40 GiB  0.01  1.00    1      up
                      TOTAL  120 GiB   61 MiB  1.6 MiB   0 B  59 MiB  120 GiB  0.01
MIN/MAX VAR: 1.00/1.00  STDDEV: 0
```

### (8) 관리 노드에서 MDS(메타 데이터 서버)를 구성

```bash
# 디렉터리 만들기
# 디렉토리 이름 ⇒ (클러스터 이름)-(노드 이름)
root@osd01:~# mkdir -p /var/lib/ceph/mds/ceph-osd01


root@osd01:~# ceph-authtool --create-keyring /var/lib/ceph/mds/ceph-osd01/keyring --gen-key -n mds.osd01


root@osd01:~# chown -R ceph. /var/lib/ceph/mds/ceph-osd01


root@osd01:~# ceph auth add mds.osd01 osd "allow rwx" mds "allow" mon "allow profile mds" -i /var/lib/ceph/mds/ceph-osd01/keyring


root@osd01:~# systemctl enable --now ceph-mds@osd01
```


### (9) Ceph fs volume 생성

```bash
root@osd01:~# ceph fs volume create kubernetes
Volume created successfully (no MDS daemons created)

root@osd01:~# ceph fs ls
name: kubernetes, metadata pool: cephfs.kubernetes.meta, data pools: [cephfs.kubernetes.data ]
```


### (10) ceph fs auth 생성

```bash
root@osd01:~# ceph auth get-or-create client.cephfs mon 'allow r' osd 'allow rwx pool=kubernetes'
[client.cephfs]
    key = AQBgJ1Rg9DMzEhAA/y3+g2tDGHOujxPVjFHS6A==
```


### (11) ceph mds stat

```bash
root@osd01:~# ceph mds stat
kubernetes:1 cephfs:1 {cephfs:0=osd01=up:active}
```

<br>