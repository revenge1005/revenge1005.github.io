---
title:  "[KUBERNETES] 06. 쿠버네티스의 오브젝트 (2)" 

categories:
  - KUBERNETES
tags:
  - [kubernetes]

toc: true
toc_sticky: true

date: 2022-03-18
last_modified_at: 2022-03-18
---
# [KUBERNETES] 05. 쿠버네티스의 오브젝트 (2)
---

<style>
table {
    font-size: 12pt;
}
table th:first-of-type {
    width: 5%;
}
table th:nth-of-type(2) {
    width: 15%;
}
table th:nth-of-type(3) {
    width: 50%;
}
table th:nth-of-type(4) {
    width: 30%;
}
</style>

<br>

## 🔔 컨트롤러 (Controller)

> 기본 오브젝트(Basic Object)를 좀 더 편리하게 관리하기 위해 사용하는 오브젝트

<br>

### 📜 리플리카셋(ReplicaSet)

> 리플리카셋은 실행되는 파드(Pod)의 개수에 대한 가용성을 보증하며, 지정한 파드의 개수 만큼 항상 실행될 수 있도록 관리하는 역할

- 즉, 5개의 파드를 항상 실행하도록 설정하면 이후 어떠한 이유로 파드가 비정상 종료 또는 삭제 될 경우 새로운 파드를 실행시켜 5개를 유지할 수 있도록 해준다.

<br>

### 📜 디플로이먼트(Deployment)

> 리플리카셋을 YAML 파일에 사용하는 경우는 거의 없고, 대부분 리플리카셋과 파드의 정보를 정의하는 디플로이먼트라는 이름의 오브젝트를 정의해서 사용한다.<br><br>
**디플로이먼트는 리플리카셋 보다 상위에 해당하는 개념**으로 리플리카셋을 똑같은 파드와 리플리카를 관리 및 제어하는 리소스라면 디플로이먼트는 리플리카셋을 관리하고 다루기 위한 리소스라고 할 수 있으며, **디플로이먼트를 생성하면 리플리카셋도 함께 생성되기 때문에 디플로이먼트는 애플리케이션 배포의 기본 단위가 된다.**

- **디플로이먼트의 Deploy 라는 단어의 뜻이 나타내는 것처럼 컨테이너 애플리케이션을 배포하고 관리하는 역할**을 한다.

- 예를 들어 애플리케이션을 업데이트할 때 리플리카셋의 변경 사항을 저장하는 리비전(Revision)을 남겨 롤백(Rollback)을 가능하게 해주고, 무증단 서비스를 위해 파드의 롤링 업데이트(Rolling Update)의 전략을 지정할 수 있다.

<br>

### 📜 잡 & 크론잡(Job & CronJob)

> **잡(Job)은 파드에 있는 모든 컨테이너가 정상적으로 종료할 때까지 재실행하는 컨트롤러**이며, **크론잡(CronJob)은 Unix의 크론과 같은 포맷으로 특정/시간에 파드를 실행시키는 등 지정한 일정에 따라 Job을 실행 시킬 수 있는 컨트롤러**이다.

- **【잡 컨트롤러의 특징】**

    |특징|설명|
    |:---:|---|
    |(1)|지정한 실행 횟수와 병행 개수에 따라 한 개 이상의 파드를 실행|
    |(2)|파드 내의 모든 컨테이너가 정상 종료한 경우에만 파드를 정상 종료한 것으로 취급|
    |(3)|잡에 기술한 파드의 실행 횟수를 전부 정상 종료하면 잡은 종료|
    |(4)|그리고 파드의 비정상 종료에 따른 재실행 횟수의 상한에 도달해도 잡은 중단|
    |(5)|노드 장애 등에 의해 잡의 파드가 제거된 경우, 다른 노드에서 파드를 재실행|
    |(6)|잡에 의해 실행된 파드는 잡이 삭제될 때까지 유지|

<br>

### 📜 스테이트풀셋(StatefulSet)

> 기존 리플리카셋, 디플로이먼트는 모두 상태가 없는(Stateless) 파드들을 관리하는 용도 였지만, **스테이트풀셋은 단어의 의미 그대로 상태를 가지고 있는 파드들을 관리하는 컨트롤러**이다. <br><br>
컨테이너나 파드는 태생적으로 데이터를 보관하는 것이 어렵기 때문에 파드와 퍼시스턴트 볼륨을 조합해 실행해야 하고 이러한 요구사항을 위해 스테이트풀셋을 제공하며 **퍼시스턴트 볼륨와 파드를 함께 조합하여 제어하기에 적합한 컨트롤러**이다. <br><br>
스테이트풀셋을 사용하면 볼륨을 사용해서 특정 데이터를 기록해두고 그걸 파드가 재시작했을때도 유지할 수 있다.

<br>

#### (a) 스테이트풀셋과 디플로이먼트 차이

##### 【파드와 퍼시스턴트 볼륨의 이름】

스테이트풀셋도 지정한 리플리카 수에 해당하는 파드를 파드 템플릿에 기술한 내용에 따라 기동되며 **스테이트풀셋에서는 파드와 퍼시스턴트 볼륨을 하나의 단위로 취급하여 동일한 번호가 이름에 부여되지만, 디플로이먼트의 경우 해시가 붙는다.**

##### 【서비스와의 연결 및 이름 해결】

스테이트풀셋 관리하의 파트의 요청을 전송하기 위한 서비스는 **대표 IP를 가지지 않는 ClusterIP의 헤드리스 모드를 사용**해야 한다. 클라이언트가 서비스의 이름으로 IP 주소를 해결하면 스테이트풀셋 관리하의 파드의 IP 주소가 랜덤하게 반환된다.

##### 【파드 분실 시 동작】

**스테이트풀셋 관리하의 파드가 노드 장애 등으로 없어진 경우, 동일한 이름으로 새롭게 파드가 기동되며, 이때 기존 파드가 사용했던 퍼시스턴트 볼륨을 이어서 사용**한다. 주의할 점은, 파드의 이름이 같아도 파드의 IP 주소는 변한다 그래서 스테이트풀셋 관리하의 파드에 접속하는 경우, 반드시 내부 DNS를 사용해서 이름을 해결해야 한다.

##### 【노드 정지 시의 동작】

**스테이트풀셋은 데이터를 분실하지 않도록 설계되어 있지만 하드웨어, 네트워크 장애로 특정 노드가 연결이 끊어졌을 때 스테이트풀셋은 새로운 파드를 기동하지 않는다.** 

가령, 노드의 상태를 관리하는 kubelet과 마스터와의 통신이 일시적으로 끊겼지만 파드는 계속해서 돌아가고 있는 경우, 마스터가 대체 파드를 기동하여 퍼시스턴트 볼륨을 마운트하게 되면 오히려 데이터가 파손될 수 있기 때문이다.

파드가 퍼시스턴트 볼륨을 마운트할 때는 엑세스 모드로 여러 노드에서 읽고/쓰기가 가능한 ReadWriteMany(RWX)와 하나의 노드에서만 읽기/쓰기할 수 있는 ReadWriteOnce(RWO)를 사용할 수 있지만, 이들은 외부 스토리지 시스템에 종속된 파라미터라, NFS와 같은 공유 가능한 퍼시스턴트 볼륨에 RWO를 설정한다고 해도 새로운 파드가 기동되는 것을 억제할 수 없다.

스테이트풀셋이 분실된 파드를 다른 노드에서 다시 기동하는 경우는 다음과 같다.

(1) 장애 노드를 k8s 클러스터의 맴버에서 제외한다.<br>
(2) 문제가 있는 파드를 강제 종료한다.<br>
(3) 장애로 인해 정지한 노드를 재기동한다.

##### 【파드 순번 제어】

스테이트풀셋의 파드 이름에 붙은 번수는 파드의 기동과 정지뿐만 아니라, 롤링 업데이트의 순서에도 사용된다. 하편, 디플로이먼트에서의 파드명은 디플로이먼트의 이름 뒤에 해시 문자열이 붙으며 파드의 기동 순서는 랜덤하게 적용된다.

(1) 시작할 때는 파드와 퍼시스턴트 볼륨을 짝지어 차례대로 기동, 정지할 때는 번호가 큰 순서로 정지 <br>
(2) 레플리카 값을 줄이면 파드 이름의 번호가 큰 것부터 삭제 <br>
(3) 롤링 업데이트할 때도 파드의 이름에 붙은 번호에 따라 갱신 

<br>

### 📜 데몬셋(DaemonSet)

> **데몬셋은 쿠버네티스를 구성하는 모든 노드에서 파드를 실행하기 위해 존재하는 컨트롤러**, 예를 들면 모니터링 시스템 구축을 위해 모든 노드에 특정 파드(로그 수집)을 관리해야 할 때 사용할 수 있다. <br><br>
**데몬셋 관리하의 파드는 k8s 클러스터의 모든 노드에서 실행되는데 k8s 클러스터에서 노드가 하나 삭제되면, 데몬셋 관리하의 파드도 그 노드에서 제거된다.**<br><br>
데몬셋을 사용하여 전체 노드가 아닌 일부 노드에만 파드를 배치하고 싶은 경우에는 노드 셀렉터를 설정하면된다.

<br>

## 🔔 쿠버네티스 오브젝트의 구성도

![그림 (1)](https://user-images.githubusercontent.com/42735894/224765304-6624894b-ff12-4965-9a3e-a768c3c8be6d.png)

![ㅇㅁㄴㅇㄴㄷㅂㅈㄷㅈㅂ](https://user-images.githubusercontent.com/42735894/224765319-a65526ae-67eb-4a26-abd3-30e111c34814.png)

<br>